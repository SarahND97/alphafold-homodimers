{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for fitting one logistic regression model to all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (precision_recall_curve, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm, itertools\n",
    "from typing import List, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd, numpy as np, seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "resultdir = \"\" # change to your resultdir\n",
    "results = pd.read_table(resultdir+\"/homodimers_logreg_features.tsv\", keep_default_na=False) # 922 ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "\n",
    "fs_opt_feat = [[\"total_interaction_area\"],[\"structural_consensus\"],[\"best_plddt_max\"],[\"best_pae_min\"],[\"hm_frac_tm0.8\"],[\"multimer_frac_tm0.8\"],[\"hm_frac_tm0.9\"],[\"multimer_frac_tm0.6\"]]\n",
    "\n",
    "nofs_opt_feat = [[\"max_iptm\"],[\"avg_iptm\"],[\"num_unique_contacts\"],[\"best_plddt_max\"],[\"best_pae_min\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_at_exact_fpr_sklearn(y_true, y_score, target_fpr=0.05, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      tpr_exact  : TPR at exactly target_fpr via linear interpolation on ROC\n",
    "      fpr_near   : nearest realized FPR on the ROC (for tolerance checks)\n",
    "      tpr_near   : TPR at that nearest realized FPR\n",
    "    \"\"\"\n",
    "    y_true  = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score, sample_weight=sample_weight, drop_intermediate=False)\n",
    "\n",
    "    # Exact TPR by interpolation (threshold-free)\n",
    "    tpr_exact = float(np.interp(target_fpr, fpr, tpr))\n",
    "\n",
    "    # Nearest realized ROC step (for band enforcement/reporting)\n",
    "    idx = int(np.argmin(np.abs(fpr - target_fpr)))\n",
    "    fpr_near = float(fpr[idx])\n",
    "    tpr_near = float(tpr[idx])\n",
    "\n",
    "    return tpr_exact, fpr_near, tpr_near\n",
    "\n",
    "def evaluate(y_true, y_score):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
    "    tpr_5, _, _ = tpr_at_exact_fpr_sklearn(y_true, y_score, 0.05)\n",
    "    tpr_1, _, _ = tpr_at_exact_fpr_sklearn(y_true, y_score, 0.01)\n",
    "    return {\n",
    "        \"tpr_at_fpr5\": tpr_5, \n",
    "        \"tpr_at_fpr1\": tpr_1, \n",
    "        }\n",
    "\n",
    "def run_logreg(feature_list, input_df):\n",
    "    df = input_df.copy()\n",
    "    labels = df[\"correct_result\"].values\n",
    "\n",
    "    # --- Normalize + flatten feature names ---\n",
    "    flat_features = []\n",
    "    for f in feature_list:\n",
    "        if isinstance(f, (list, tuple, np.ndarray)):\n",
    "            for g in f:\n",
    "                if isinstance(g, tuple) and len(g) == 1:\n",
    "                    flat_features.append(g[0])\n",
    "                else:\n",
    "                    flat_features.append(g)\n",
    "        else:\n",
    "            flat_features.append(f)\n",
    "\n",
    "    norm_features = [str(x) for x in flat_features]\n",
    "\n",
    "    # (same sanity check as before if you want, or reuse your normalize_feature_list)\n",
    "    cols = set(df.columns)\n",
    "    missing = [f for f in norm_features if f not in cols]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"The following requested features are not in df.columns:\\n\"\n",
    "            f\"{sorted(set(missing))}\\n\\n\"\n",
    "            f\"Available columns include (first 30):\\n{list(df.columns)[:30]}\"\n",
    "        )\n",
    "\n",
    "    # ⬇️ KEY LINE: keep X as a DataFrame\n",
    "    X = df[norm_features]\n",
    "\n",
    "    pipe = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(max_iter=1000, solver=\"lbfgs\", n_jobs=1, random_state=42),\n",
    "    )\n",
    "    pipe.fit(X, labels)\n",
    "\n",
    "    proba = pipe.predict_proba(X)[:, 1]\n",
    "\n",
    "    # ... rest of the function unchanged ...\n",
    "    per_pdb_rows = []\n",
    "    pdb_ids = df[\"clean_query\"].values\n",
    "    for pdb, yt, ys in zip(pdb_ids, labels, proba):\n",
    "        per_pdb_rows.append({\n",
    "            \"pdbid\": pdb,\n",
    "            \"y_true\": int(yt),\n",
    "            \"y_proba\": float(ys),\n",
    "        })\n",
    "\n",
    "    per_pdb_df = pd.DataFrame(per_pdb_rows)\n",
    "    return per_pdb_df, pipe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nofseek_df, nofseek_model  = run_logreg(nofs_opt_feat, results)\n",
    "fseek_df,fseek_model = run_logreg(fs_opt_feat, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NoFoldseek model feature weights ===\n",
      "avg_iptm               1.610355\n",
      "best_plddt_max         0.292402\n",
      "num_unique_contacts    0.081868\n",
      "best_pae_min          -0.244378\n",
      "max_iptm              -0.351177\n",
      "dtype: float64\n",
      "Intercept: 0.09456490639862897\n",
      "\n",
      "=== Foldseek model feature weights ===\n",
      "hm_frac_tm0.8             1.196562\n",
      "best_plddt_max            0.735575\n",
      "hm_frac_tm0.9             0.407898\n",
      "multimer_frac_tm0.6       0.323644\n",
      "structural_consensus      0.225121\n",
      "total_interaction_area    0.213918\n",
      "best_pae_min             -0.602685\n",
      "multimer_frac_tm0.8      -0.891486\n",
      "dtype: float64\n",
      "Intercept: 0.16108041276809557\n"
     ]
    }
   ],
   "source": [
    "# Weight of each feature:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_feature_list(feature_list, df):\n",
    "    \"\"\"Flatten + normalize features in the same way as run_logreg.\"\"\"\n",
    "    flat_features = []\n",
    "    for f in feature_list:\n",
    "        if isinstance(f, (list, tuple, np.ndarray)):\n",
    "            for g in f:\n",
    "                if isinstance(g, tuple) and len(g) == 1:\n",
    "                    flat_features.append(g[0])\n",
    "                else:\n",
    "                    flat_features.append(g)\n",
    "        else:\n",
    "            flat_features.append(f)\n",
    "\n",
    "    norm_features = [str(x) for x in flat_features]\n",
    "\n",
    "    # (Optional) sanity check – same as in run_logreg\n",
    "    cols = set(df.columns)\n",
    "    missing = [f for f in norm_features if f not in cols]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"The following requested features are not in df.columns:\\n\"\n",
    "            f\"{sorted(set(missing))}\\n\\n\"\n",
    "            f\"Available columns include (first 30):\\n{list(df.columns)[:30]}\"\n",
    "        )\n",
    "\n",
    "    return norm_features\n",
    "\n",
    "\n",
    "def get_feature_weights(model, feature_list, df):\n",
    "    \"\"\"\n",
    "    Return a sorted Series of feature weights for a fitted pipeline model.\n",
    "    \"\"\"\n",
    "    norm_features = normalize_feature_list(feature_list, df)\n",
    "    logreg = model.named_steps[\"logisticregression\"]\n",
    "    coefs = logreg.coef_[0]  # shape (n_features,)\n",
    "\n",
    "    weights = pd.Series(coefs, index=norm_features).sort_values(ascending=False)\n",
    "    intercept = float(logreg.intercept_[0])\n",
    "    return weights, intercept\n",
    "\n",
    "\n",
    "# ---- For your two models ----\n",
    "nofseek_weights, nofseek_intercept = get_feature_weights(\n",
    "    nofseek_model, nofs_opt_feat, results\n",
    ")\n",
    "fseek_weights, fseek_intercept = get_feature_weights(\n",
    "    fseek_model, fs_opt_feat, results\n",
    ")\n",
    "\n",
    "print(\"=== NoFoldseek model feature weights ===\")\n",
    "print(nofseek_weights)\n",
    "print(\"Intercept:\", nofseek_intercept)\n",
    "\n",
    "print(\"\\n=== Foldseek model feature weights ===\")\n",
    "print(fseek_weights)\n",
    "print(\"Intercept:\", fseek_intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fseek_logreg.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": nofseek_model,\n",
    "        \"features\": nofs_opt_feat,\n",
    "    },\n",
    "    \"nofseek_logreg.joblib\",\n",
    ")\n",
    "\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": fseek_model,\n",
    "        \"features\": fs_opt_feat,\n",
    "    },\n",
    "    \"fseek_logreg.joblib\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "function_dir = \"\" # specify where the files are located\n",
    "nofseek_bundle = joblib.load(function_dir+\"nofseek_logreg.joblib\")\n",
    "nofseek_model_loaded = nofseek_bundle[\"model\"]\n",
    "nofs_opt_feat_loaded = nofseek_bundle[\"features\"]\n",
    "\n",
    "fseek_bundle = joblib.load(function_dir+\"fseek_logreg.joblib\")\n",
    "fseek_model_loaded = fseek_bundle[\"model\"]\n",
    "fs_opt_feat_loaded = fseek_bundle[\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use saved models, here using the same data again just to show usage: \n",
    "\n",
    "nofs_features_norm = normalize_feature_list(nofs_opt_feat_loaded, results)\n",
    "fs_features_norm   = normalize_feature_list(fs_opt_feat_loaded, results)\n",
    "\n",
    "# ⬇️ IMPORTANT: no .values here, keep as DataFrame\n",
    "X_nofs = results[nofs_features_norm]\n",
    "X_fs   = results[fs_features_norm]\n",
    "\n",
    "proba_nofs = nofseek_model_loaded.predict_proba(X_nofs)[:, 1]\n",
    "proba_fs   = fseek_model_loaded.predict_proba(X_fs)[:, 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
